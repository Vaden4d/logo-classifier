{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "built-scope",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-passenger",
   "metadata": {},
   "source": [
    "1. Preprocessing\n",
    "\n",
    "See **Preprocessing.py** file. I decided to make preprocessing in the Jupyter Notebook only, because I always make additional exploration of the data and need interactivity. The goals of preprocessing - clean the data to avoid data leakage, delete duplicates and generate possible labels for classifier training. The output labels after this process I called weak labels. The output of preprocessing - .csv file with pathes to the images and its weak labels. \n",
    "\n",
    "2. Evaluation\n",
    "\n",
    "Also see in **Preprocessing.py**. Shortly, the precision is key metric here, however, precision should be validated on the labeled part. During the training, I observe F1 over weak labels, and after training script outputs Precision and Recall over real labeled part. \n",
    "\n",
    "2. Models and approaches\n",
    "\n",
    "For classification, I used **EfficientNet-B1** using supervised and semi-supervised settings. It's golden standard of the classification, so the good start point. I didn't make any updates of its architecture, except the last FC layer with custom bias initialization using weakly labeled part of the data via the rule:\n",
    "\n",
    "<center> $b_i = \\log(\\frac{n_i}{n})$, where $\\frac{n_i}{n}$ - estimate of the probability of class $i$ </center>\n",
    "<br>\n",
    "As good approach to handle logo classification with a lot of trash, I considered semi-supervised learning. Because we have a lot of data, and even weak labels can cover only few percents of the data. So a lot of data aren't used in supervised setting, and semi-supervised approach looks more desired here. I used MixMatch approach - it's simple and need only to change the data loaders and loss function. I used existed GitHub implementation and united it with my code uses PyTorch Lightning. Validation of the semi-supervised model is performed similarly as in the supervised setting.\n",
    "<br>\n",
    "<br>\n",
    "Also, to handle non-correct labels in the data (both weak and manually labeled parts), I used label smoothing united with Cross Entropy. Approaches like Cross Entropy with weights for some classes don't work during my experiments.\n",
    "\n",
    "3. Training\n",
    "\n",
    "For training, I prepared training pipeline with the main file **train.py**. It uses some functions from the package, and has simple customizations over input parameters. I used functionality of **PyTorch Lightning** and defined only several parts of the training loop, like models and Trainer object. In the end of training, I printed metrics over test sample and the same metrics over the validation part.\n",
    "\n",
    "The training results I displayed in the README table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-adoption",
   "metadata": {},
   "source": [
    "## Possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-navigation",
   "metadata": {},
   "source": [
    "During my dataset exploration, I saw several types of non-correct logo detections:\n",
    "\n",
    "1) Crops over random text, it typically has low entropy and only text over all image\n",
    "2) Crops over real objects, like cars, human bodies, etc\n",
    "3) Real logos\n",
    "\n",
    "So 2) can be solved using anomaly detection, because it doesn't belong to typical data elements of the dataset, like real logo or images with text. However, to solve 1), anomaly detection isn't appropriate and there we need to extract text information from the data. I haven't made it yet  because a lot of texts in the dataset isn't in English, so Tesseract need to be configured for other languages. \n",
    "\n",
    "Of course, the step with text preprocessing is strongly connected to the weak labels generation - it should help to increase the confidence of generated labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
